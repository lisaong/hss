{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "colab": {
      "name": "05_physionet.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lisaong/hss/blob/master/05_physionet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57_kyL7UvrAa",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "Name:\n",
        "\n",
        "Date:\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESjbriLA_z0d",
        "colab_type": "text"
      },
      "source": [
        "# Sensor Data Analysis\n",
        "\n",
        "Analyse \"Bag of sensors\" data from PhysioNet: https://physionet.org/physiobank/database/noneeg/\n",
        "\n",
        "Under/Oversampling to align samples\n",
        "\n",
        "Feature Extraction\n",
        "- Statistical\n",
        "- Spectral\n",
        "\n",
        "Machine Learning\n",
        "- Time-domain features\n",
        "- Frequency-domain features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcBBA2z7_z0g",
        "colab_type": "text"
      },
      "source": [
        "## Data Introduction\n",
        "\n",
        "This database contains non-EEG physiological signals collected at Quality of Life Laboratory at University of Texas at Dallas, used to infer the neurological status (including physical stress, cognitive stress, emotional stress and relaxation) of 20 healthy subjects. The data was collected using non-invasive wrist worn biosensors and consists of electrodermal activity (EDA), temperature, acceleration, heart rate (HR), and arterial oxygen level (SpO2).\n",
        "\n",
        "The experimental procedures involving human subjects described in this work were approved under UTD IRB # 12-29 by the Institutional Review Board at the University of Texas at Dallas, Richardson, Texas, USA. \n",
        "\n",
        "The dataset consists of 7 stages for 20 subjects: \n",
        "\n",
        "- First Relaxation: five minutes \n",
        "- Physical Stress: Stand for one minute, walk on a treadmill at one mile per hour for two minutes, then walk/jog on the treadmill at three miles per hour for two minutes. \n",
        "- Second Relaxation: five minutes\n",
        "- Mini-emotional stress: 40 seconds (Note: This portion of the data, which was collected right before the cognitive stress task, is not explained in the paper.) During this 40 seconds, the “instructions” for the math portion of the cognitive stress (to count backwards by sevens, beginning with 2485, for three minutes) were read to the volunteer. \n",
        "- Cognitive Stress: Count backwards by sevens, beginning with 2485, for three minutes. Next, perform the Stroop test for two minutes. The volunteer was alerted to errors by a buzzer. The Stroop test consisted of reading the names of colors written in a different color ink, then saying what color the ink was. \n",
        "- Third Relaxation: five minutes. \n",
        "- Emotional Stress: The volunteer was told he/she would be shown a five minute clip from a horror movie in one minute. After the minute of anticipation, a clip from a zombie apocalypse movie, The Horde was shown. \n",
        "- Fourth Relaxation: five minutes. \n",
        "\n",
        "Note: We had not originally intended to count the reading of the instructions to count backwards as an emotional stress. After all, instructions were given for each of the tasks. Unlike the other instruction sets, however, this one created a stress response in many of the volunteers that was obvious to the test administrator as the test was being given."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zexEg6bY_z0i",
        "colab_type": "text"
      },
      "source": [
        "## Load Data\n",
        "\n",
        "Each subject has several datafiles:\n",
        "- SubjectN_AccTempEDA.atr: annotation\n",
        "- SubjectN_AccTempEDA.dat: data\n",
        "- SubjectN_AccTempEDA.hea: header\n",
        "- SubjectN_Sp02HR.dat: data\n",
        "- SubjectN_Sp02HR.hea: header\n",
        "\n",
        "These files are in the WFDB (WaveForm DataBase) format, and can be read using the `wfdb` python module.\n",
        "(https://github.com/MIT-LCP/wfdb-python)\n",
        "\n",
        "https://www.physionet.org/standards/npsg/Moody.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUqTx5HHBwdo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download and extract the data files\n",
        "!wget -q -O Subject10.zip https://github.com/lisaong/hss/raw/colab/data/physionet/Subject10.zip\n",
        "!unzip -o Subject10.zip\n",
        "!rm Subject10.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKLFK5QuDXFe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install the Waveform Database library to read the data files\n",
        "!pip install wfdb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYOJGtsw_z0j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from statsmodels.graphics.tsaplots import plot_acf\n",
        "from scipy import fftpack, signal\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "sns.set_style('whitegrid') # style\n",
        "\n",
        "import wfdb\n",
        "\n",
        "# render plots inline\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_UQ4Gb2_z0l",
        "colab_type": "text"
      },
      "source": [
        "### Acc Temp EDA\n",
        "\n",
        "Accelerometer, Temperature, Electrodermal Activity Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iR1v3lB3_z0m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read annotations\n",
        "ann = wfdb.rdann('./Subject10_AccTempEDA', extension='atr', summarize_labels=True)\n",
        "print(ann.__dict__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EAX0OqA_z0r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read records\n",
        "record_acc_temp_eda = wfdb.rdrecord('./Subject10_AccTempEDA')\n",
        "print(record_acc_temp_eda.__dict__)\n",
        "\n",
        "wfdb.plot_wfdb(record=record_acc_temp_eda, title='Subject10_AccTempEDA', annotation=ann, plot_sym=True, \n",
        "               time_units='seconds', figsize=(15, 10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c49GzWNr_z0t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_acc_temp_eda = record_acc_temp_eda.p_signal\n",
        "data_acc_temp_eda.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Es0zX_sX_z0v",
        "colab_type": "text"
      },
      "source": [
        "### SpO2 HR\n",
        "\n",
        "Arterial Oxygen Levels, Heartrate\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dc0e8Khp_z0w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "record_spo2_hr = wfdb.rdrecord('./Subject10_SpO2HR')\n",
        "print(record_spo2_hr.__dict__)\n",
        "\n",
        "wfdb.plot_wfdb(record=record_spo2_hr, title='Subject10_SpO2HR', time_units='seconds', figsize=(15, 5))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ym1fjd4g_z0y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_spo2_hr = record_spo2_hr.p_signal\n",
        "data_spo2_hr.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huL4tDOm_z00",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# number of acceleration, etc samples per second\n",
        "record_acc_temp_eda.fs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMLq5Luo_z02",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# number of SpO2 and HR samples per second\n",
        "record_spo2_hr.fs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sawm6W3M_z06",
        "colab_type": "text"
      },
      "source": [
        "## Aligning data of different frequencies\n",
        "\n",
        "The two dataset frequencies (number of samples per second) are different.\n",
        "\n",
        "To support processing both datasets at the same time, we need to match the frequencies.\n",
        "\n",
        "This is a common situation when taking readings from different sensors or data sources.\n",
        "\n",
        "Two strategies:\n",
        "1. Upsampling the smaller frequency data. E.g: repeat samples or interpolate.\n",
        "2. Downsampling the larger frequency data. E.g: replace with mean or median.\n",
        "\n",
        "Which one to pick depends on requirements: whether you need to maintain precision of the higher frequency dataset.\n",
        "\n",
        "Example: https://machinelearningmastery.com/resample-interpolate-time-series-data-python/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9WdcizJ_z06",
        "colab_type": "text"
      },
      "source": [
        "### Option 1: Upsampling SpO2 HR to 8 samples per second\n",
        "\n",
        "This option duplicates the SpO2 HR data at the smaller sampling interval."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nR4q-2rB_z07",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create an index with 1 second timestamps, using the length of data_spo2_hr\n",
        "# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.period_range.html\n",
        "#\n",
        "# frequency strings: https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html\n",
        "\n",
        "# for this dataset, the start date is just an arbitrary reference so that we can\n",
        "# use resample()\n",
        "per_second_index = pd.period_range(start='2019-01-01', periods=len(data_spo2_hr), freq='S')\n",
        "per_second_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfXgLw24_z09",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a dataframe for SpO2 data using the above period index\n",
        "df_spO2_hr = pd.DataFrame(data_spo2_hr, index=per_second_index, columns=record_spo2_hr.sig_name)\n",
        "df_spO2_hr.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9G7pJ-Pz_z0_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# upsample to match the frequency of the other data (8 times)\n",
        "\n",
        "samples_per_sec = record_acc_temp_eda.fs // record_spo2_hr.fs # // converts float to int (ceiling)\n",
        "samples_per_sec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3Vgf4Wd_z1D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# resample, then interpolate\n",
        "# Note: whether interpolation makes sense depends on the sensor and use case\n",
        "upsampled = df_spO2_hr.resample('125ms')\n",
        "\n",
        "df_upsampled = upsampled.interpolate()\n",
        "df_upsampled.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgjX-VxM_z1F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# inspect the resulting dataframe\n",
        "# Note the number of rows has increased by a factor of 8\n",
        "df_upsampled.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67a3ekly_z1J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Note: there are fewer values in the Acc dataframe, so we need to ignore the\n",
        "# later entries from df_upsampled.\n",
        "\n",
        "df_acc_temp_eda = pd.DataFrame(data_acc_temp_eda, columns=record_acc_temp_eda.sig_name)\n",
        "df_acc_temp_eda.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDy53ZsK_z1N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# skip the first (18288-18239 = 49) entries\n",
        "df_acc_temp_eda.index = df_upsampled.index[:len(df_acc_temp_eda)]\n",
        "df_acc_temp_eda.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joIh_s35_z1S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# concatenate the two dataframes, column-wise\n",
        "# we now have Acc, Temp, EDA, SpO2, HR in 1 combined dataframe\n",
        "df_option1 = pd.concat([df_acc_temp_eda, df_upsampled], axis=1).dropna()\n",
        "df_option1.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSCMsXWB_z1U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# inspect the number of columns, rows, and datatype\n",
        "df_option1.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWNv7NGy_z1X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot the data\n",
        "# https://stackoverflow.com/questions/48126330/python-int-too-large-to-convert-to-c-long-plotting-pandas-dates\n",
        "df_option1.index = pd.to_datetime(df_option1.index.to_timestamp())\n",
        "\n",
        "df_option1.plot(figsize=(15, 10))\n",
        "ax = plt.gca()\n",
        "ax.set_title('Upsampled Data')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItNryZmU_z1c",
        "colab_type": "text"
      },
      "source": [
        "### Option 2: Downsampling Acc Temp EDA to 1 sample per second\n",
        "\n",
        "In the previous option, we duplicate SpO2 HR data to the smaller interval. \n",
        "\n",
        "<font color='red'>**Q1: What problems do you think this causes?**</font>\n",
        "\n",
        "```\n",
        "Enter your answer here:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqiISgysvjkL",
        "colab_type": "text"
      },
      "source": [
        "An alternative approach is to sample the Acc Temp EDA data to match the larger interval of SpO2 HR.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWIbuGZf_z1d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create an index with 125 millisecond timestamps, using the length of data_acc_temp_eda\n",
        "#\n",
        "# frequency strings: https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html\n",
        "\n",
        "# for this dataset, the start date is just an arbitrary reference\n",
        "per_125_ms_index = pd.period_range(start='2019-01-01', periods=len(data_acc_temp_eda), freq='125ms')\n",
        "per_125_ms_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WUhU4lK_z1e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a dataframe for Acc Temp EDA using the 125ms period index\n",
        "df_acc_temp_eda2 = pd.DataFrame(data_acc_temp_eda, index=per_125_ms_index, columns=record_acc_temp_eda.sig_name)\n",
        "df_acc_temp_eda2.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfec_YZ6_z1i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# downsample using median of Acc Temp EDA2\n",
        "# using median is safer than using mean because median is less susceptible to outliers\n",
        "# Can you think of the reason why median is less susceptible to outliers?\n",
        "\n",
        "df_acc_temp_eda_downsampled = df_acc_temp_eda2.resample('S').median()\n",
        "df_acc_temp_eda_downsampled.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gl0oPEl-_z1k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# inspected the downsampled data\n",
        "# Note the resulting number of rows has decreased by a factor of 8\n",
        "df_acc_temp_eda_downsampled.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHmF_2r5_z1o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a dataframe so that we can concatenate\n",
        "df_spo2_hr2 = pd.DataFrame(data_spo2_hr, columns=record_spo2_hr.sig_name, index=per_second_index)\n",
        "df_spo2_hr2.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psg05sZm_z1r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# concatenate the two dataframes, column-wise\n",
        "df_option2 = pd.concat([df_acc_temp_eda_downsampled, df_spo2_hr2], axis=1).dropna()\n",
        "df_option2.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5YQOtuP_z1t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# inspect the resulting dataframe of Acc Temp EDA SpO2 HR data\n",
        "df_option2.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SaC5sbHO_z1x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot the data\n",
        "df_option2.index = pd.to_datetime(df_option2.index.to_timestamp())\n",
        "\n",
        "df_option2.plot(figsize=(15, 10))\n",
        "ax = plt.gca()\n",
        "ax.set_title('Downsampled Data')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfTBw_xaW8PR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's plot accelerometerX side-by-side\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(20, 10))\n",
        "\n",
        "# zoom in to any 1 second time window\n",
        "start_time = '2019-01-01 00:15'\n",
        "end_time = '2019-01-01 00:16'\n",
        "\n",
        "column = 'ax'\n",
        "\n",
        "ax.plot(df_option1[(df_option1.index >= start_time) & (df_option1.index < end_time)][column],\n",
        "        label='Upsampled (with interpolation)')\n",
        "ax.plot(df_option2[(df_option2.index >= start_time) & (df_option2.index < end_time)][column],\n",
        "        label='Downsampled (with median)')\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xruU9MFHZlCp",
        "colab_type": "text"
      },
      "source": [
        "Generate plots for other columns, such as ay, temp, SpO2. \n",
        "\n",
        "<font color='red'>**Q2: What do you observe if you compare the upsampled and downsampled SpO2 or HR? Why?**</font>\n",
        "\n",
        "```\n",
        "Enter your answer here:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIEnJSj0_z13",
        "colab_type": "text"
      },
      "source": [
        "## Statistical Features\n",
        "\n",
        "Basic statistical features you can get from time-series data, using Pandas and other python libraries:\n",
        "\n",
        "- Mean, median, standard deviation\n",
        "- Quantisation / discretisation\n",
        "- Correlation\n",
        "- Auto-correlation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APs5_4Sr_z15",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df_option1.copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1WicEeo_z16",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.mean() # mean of each column"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_mujsNM_z19",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.median() # median is less sensitive to outliers than mean"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kd0uX__l_z2A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.std() # standard deviation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-0mSseO_z2D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.max()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WDin1GJ_z2E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.min()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RFuAGJb_z2H",
        "colab_type": "text"
      },
      "source": [
        "### Discretise into quantiles\n",
        "\n",
        "Another technique with Time Series data is to apply Discretisation.  Discretisation is useful when there is a lot of noise in the signal and you want to create models that work more generically (on \"bins\" of input values).\n",
        "\n",
        "https://datascience.stackexchange.com/questions/19782/what-is-the-rationale-for-discretization-of-continuous-features-and-when-should"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ithZnmk_z2I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.ax.values.ravel() # raw values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLGzh1eh_z2J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# discretise using qcut, 10 bins\n",
        "# duplicates='drop' drops overlapping edges\n",
        "df['ax_q10'] = pd.qcut(df.ax.values.ravel(), 10, duplicates='drop')\n",
        "df.ax_q10.value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwyoBlAD_z2M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['ax_q10']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLGqDcIy_z2N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['ay_q10'] = pd.qcut(df.ay.values.ravel(), 10, duplicates='drop')\n",
        "df['az_q10'] = pd.qcut(df.az.values.ravel(), 10, duplicates='drop')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhOqoTWC_z2P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.ay_q10.value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOZV31_xdBFX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.az_q10.value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uO43srjJ_z2Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLwdx1uC_z2R",
        "colab_type": "text"
      },
      "source": [
        "### Pair-plot\n",
        "\n",
        "Pair plots are a combination of scatter plots and histograms. \n",
        "\n",
        "They are done for each pair of features (e.g. ax vs. ay)\n",
        "\n",
        "https://seaborn.pydata.org/generated/seaborn.pairplot.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gisuLCGx_z2S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.pairplot(df)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJE8ohQVdVBS",
        "colab_type": "text"
      },
      "source": [
        "1. What do you notice about temp vs. EDA? **Are they positively or negatively correlated?**\n",
        "2. Do you notice any **outliers** in the data? If yes, can you think of a simple way to remove outliers? (hint: something that uses the simple statistical features)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACvy_H2x_z2U",
        "colab_type": "text"
      },
      "source": [
        "### Correlation\n",
        "\n",
        "Correlations provide a metric to indicate whether two variables are strongly dependent.\n",
        "\n",
        "https://www.statisticssolutions.com/correlation-pearson-kendall-spearman/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xc3ItffQ_z2W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.corr(method='pearson')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pub70Z_Q_z2X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# a graphical way to viewing the correlation matrix\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "sns.heatmap(df.corr(method='pearson'), annot=True, fmt='.2f')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9F3T4Jw_z2e",
        "colab_type": "text"
      },
      "source": [
        "## Spectal Features\n",
        "\n",
        "Another way to extract features from time series data is to decompose the time series into its frequency components.\n",
        "\n",
        "A common technique is to use Fast Fourier Transforms:\n",
        "\n",
        "https://flothesof.github.io/Fourier-series-rectangle.html\n",
        "\n",
        "https://ipython-books.github.io/101-analyzing-the-frequency-components-of-a-signal-with-a-fast-fourier-transform/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FzuhwDU_z2f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Helper function to compute power spectral density\n",
        "def power_spectral_density(x):\n",
        "    psd = np.abs(x) ** 2\n",
        "    return 20 * np.log10(psd / psd.max()) # decibels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmnoN0e3_z2g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compute fft of some signals\n",
        "fft_ax = fftpack.fft(df.ax)\n",
        "fft_hr = fftpack.fft(df.hr)\n",
        "fft_eda = fftpack.fft(df.EDA)\n",
        "# can add other signals as well\n",
        "\n",
        "# Extract the power spectral density (squared magnitude of FFT, in decibels)\n",
        "psd_ax = power_spectral_density(fft_ax)\n",
        "psd_hr = power_spectral_density(fft_hr)\n",
        "psd_eda = power_spectral_density(fft_eda)\n",
        "\n",
        "sample_freq = fftpack.fftfreq(len(df.ax), d=1./samples_per_sec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wz68WuP2_z2j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot the PSD of each FFT, focusing only on the positive frequencies\n",
        "pos_mask = sample_freq > 0\n",
        "\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(nrows=3, ncols=1, figsize=(15, 15))\n",
        "\n",
        "ax1.plot(sample_freq[pos_mask], psd_ax[pos_mask])\n",
        "ax1.set_title('ax')\n",
        "ax1.set_ylabel('PSD (dB)')\n",
        "ax1.set_xlabel('Frequency (Hz)')\n",
        "\n",
        "ax2.plot(sample_freq[pos_mask], psd_hr[pos_mask])\n",
        "ax2.set_title('hr')\n",
        "ax2.set_ylabel('PSD (dB)')\n",
        "ax2.set_xlabel('Frequency (Hz)')\n",
        "\n",
        "ax3.plot(sample_freq[pos_mask], psd_eda[pos_mask])\n",
        "ax3.set_title('EDA')\n",
        "ax3.set_ylabel('PSD (dB)')\n",
        "ax3.set_xlabel('Frequency (Hz)')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTNpIZIA_z2l",
        "colab_type": "text"
      },
      "source": [
        "### Smoothing\n",
        "\n",
        "The above FFTs are very noisy, because we are reading individual samples.\n",
        "\n",
        "Let's try to apply a window technique to smooth out the samples.\n",
        "\n",
        "An example is Hamming Window:\n",
        "https://flothesof.github.io/FFT-window-properties-frequency-analysis.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYWMhF39_z2m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "m = samples_per_sec * 5 # window size\n",
        "w = signal.get_window('hamming', m)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(w)\n",
        "ax.set_xlabel('sample #')\n",
        "ax.set_ylabel('amplitude')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbQ25g44_z2n",
        "colab_type": "text"
      },
      "source": [
        "The above is a hamming window of 5 seconds (8 samples per second).\n",
        "\n",
        "Now, let's apply the windowing function to our signals, and then take the FFT.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udFzUvOw_z2n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# transform the signal into windows\n",
        "smoothed_window = df.ax[:m] * w\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(df.ax[:m], label='before smoothing')\n",
        "ax.plot(smoothed_window, label='after smoothing')\n",
        "ax.legend()\n",
        "ax.set_title('Hamming Window Illustration (5 seconds)')\n",
        "fig.autofmt_xdate()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoNqrC_9gw-p",
        "colab_type": "text"
      },
      "source": [
        "<font color='red'>**Q3: What is the difference between the 'after smoothing' plot compared to 'before smoothing'?**</font>\n",
        "\n",
        "(Hint: look at the Hamming Window function's amplitude plot) \n",
        "\n",
        "```\n",
        "Enter your answer here:\n",
        "\n",
        "\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-DqPJ0Q_z2q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Apply a rolling window transform onto the whole signal\n",
        "df_smoothed = df.rolling(m, win_type='hamming').mean().dropna()\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(15, 5))\n",
        "ax.plot(df.ax[m-1:], label='before smoothing')\n",
        "ax.plot(df_smoothed.ax, label='after smoothing')\n",
        "ax.set_title('Hamming Window for ax (whole series)')\n",
        "\n",
        "fig.autofmt_xdate()\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37MgWFWe_z2r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Recompute FFT on the smoothed signals\n",
        "\n",
        "fft_ax = fftpack.fft(df_smoothed.ax)\n",
        "fft_hr = fftpack.fft(df_smoothed.hr)\n",
        "fft_eda = fftpack.fft(df_smoothed.EDA)\n",
        "\n",
        "psd_ax = power_spectral_density(fft_ax)\n",
        "psd_hr = power_spectral_density(fft_hr)\n",
        "psd_eda = power_spectral_density(fft_eda)\n",
        "\n",
        "sample_freq = fftpack.fftfreq(len(df_smoothed.ax), d=1./samples_per_sec)\n",
        "\n",
        "pos_mask = sample_freq > 0\n",
        "\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(nrows=3, ncols=1, figsize=(15, 15))\n",
        "\n",
        "ax1.plot(sample_freq[pos_mask], psd_ax[pos_mask])\n",
        "ax1.set_title('ax (smoothed)')\n",
        "ax1.set_ylabel('PSD (dB)')\n",
        "ax1.set_xlabel('Frequency (Hz)')\n",
        "\n",
        "ax2.plot(sample_freq[pos_mask], psd_hr[pos_mask])\n",
        "ax2.set_title('hr (smoothed)')\n",
        "ax2.set_ylabel('PSD (dB)')\n",
        "ax2.set_xlabel('Frequency (Hz)')\n",
        "\n",
        "ax3.plot(sample_freq[pos_mask], psd_eda[pos_mask])\n",
        "ax3.set_title('EDA (smoothed)')\n",
        "ax3.set_ylabel('PSD (dB)')\n",
        "ax3.set_xlabel('Frequency (Hz)')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGc6ekHC_z2t",
        "colab_type": "text"
      },
      "source": [
        "## Machine Learning\n",
        "\n",
        "In the next section, we can explore machine learning using first the time domain signal (with discretised features), and then later on using the frequency-domain features.\n",
        "\n",
        "- Encode discretised features\n",
        "- Add and encode labels\n",
        "- Train an \"emotional state\" classification model\n",
        "  - Classification using time-domain signal\n",
        "  - Classification using frequency-domain features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zD6i2c9kS14",
        "colab_type": "text"
      },
      "source": [
        "### Encode discretised features\n",
        "\n",
        "To add the discretised features, we can map the bins to categorical values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjaZUW5-k60u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "le_ax_q10 = LabelEncoder()\n",
        "df['ax_q10_enc'] = le_ax_q10.fit_transform(df['ax_q10'])\n",
        "\n",
        "le_ay_q10 = LabelEncoder()\n",
        "df['ay_q10_enc'] = le_ay_q10.fit_transform(df['ay_q10'])\n",
        "\n",
        "le_az_q10 = LabelEncoder()\n",
        "df['az_q10_enc'] = le_az_q10.fit_transform(df['az_q10'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sppfSG0Q_z2u",
        "colab_type": "text"
      },
      "source": [
        "### Add and Encode Emotional State labels\n",
        "\n",
        "Recall that the data was collected while the person is performing some relaxation / stressful activities.\n",
        "\n",
        "The labels can be used to train a machine learning model to predict whether the sensor signals (HR, Acc, Temp) indicate stress or relaxation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymerMOig_z2u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# find where the labels are stored in our dataset\n",
        "ann.__dict__"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQgM4BSI_z2v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# find which rows are the labels in the data series\n",
        "ann.sample"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7nV6BJY_z2w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ann.aux_note"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toHMaQZe_z2x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# align the labels with the row offsets\n",
        "# (row indices are 0-based)\n",
        "start = ann.sample - 1\n",
        "end = ann.sample[1:] - 1\n",
        "end[-1] = -1 # last element\n",
        "\n",
        "print('start indices', start)\n",
        "print('end indices', end)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXYV7FDH_z20",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a new column with empty labels\n",
        "df['label'] = ''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JV7SxBeJ_z24",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# mark the labels based on the start and end offsets\n",
        "for s, e, label in zip(start, end, ann.aux_note):\n",
        "    print(s, e, label) # s is inclusive, e is exclusive\n",
        "    df.loc[s:e, 'label'] = label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffA3KXAVjVzY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.tail()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFEG7Tw6_z27",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set the final row's label to the second-last row's label\n",
        "df.loc[df.index[-1], 'label'] = df.loc[df.index[-2], 'label']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3SrXYov_z28",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.tail()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwnTU3Vm_z2-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Label Encode\n",
        "\n",
        "le = LabelEncoder()\n",
        "df['label_enc'] = le.fit_transform(df['label'])\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZADErlxU_z3A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Check correlation after encoding\n",
        "df.corr()['label_enc']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTumEVqApcrE",
        "colab_type": "text"
      },
      "source": [
        "<font color='red'>**Q4: Based on the correlations, what top 5 features will you pick, and why?**</font>\n",
        "\n",
        "```\n",
        "Enter your answer here:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBY7mr93_z3F",
        "colab_type": "text"
      },
      "source": [
        "### Shuffle and Train-test split\n",
        "\n",
        "In machine learning, the customary practice is to set apart some data for independent testing.\n",
        "\n",
        "The objective is to ensure that the resulting model is **generalisable**, meaning that it work well for data that is not seen before. This also detects a problem known as **overfitting**, where the learnt model is too specialised for the data it has seen. Typically this happens for more complicated models where you can overtrain."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVNpZSs5_z3F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train, df_test = train_test_split(df, random_state=42) # randomise features the same way each time (repeatability)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FxlpOG5_z3I",
        "colab_type": "text"
      },
      "source": [
        "### Feature Selection\n",
        "\n",
        "Previously, we used correlation as a way to decide which features may work \"better\" for predicting the target.\n",
        "\n",
        "This test can be automated using Select K-best features, which computes correlation of each candidate feature with the target."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gtvlofez_z3I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# exclude the label, encoded label, and the quantised intervals\n",
        "candidate_features = df.columns.drop(['label', 'label_enc', 'ax_q10', 'ay_q10', 'az_q10'])\n",
        "candidate_features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DMOz-rO_z3K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we arbitrarily pick k=5 as a starting point. This can be tuned later on.\n",
        "kbest = SelectKBest(k=5, score_func=f_classif)\n",
        "kbest.fit(df_train.loc[:, candidate_features], df_train.label_enc)\n",
        "\n",
        "kbest.scores_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ozp4Wz0_z3K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# best 5 features (note that this is not in k-score ordering, but in \n",
        "# the original order of the columns)\n",
        "candidate_features[kbest.get_support()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ln97-1jXrCV-",
        "colab_type": "text"
      },
      "source": [
        "Compare the best 5 features from kbest against actual correlation from the cell below:\n",
        "\n",
        "<font color='red'>**Q5: How do the best 5 features from kbest relate to the Actual correlation values in the next cell?**</font>\n",
        "\n",
        " ```\n",
        "Enter your answer here:\n",
        "\n",
        "\n",
        "\n",
        " ```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inyrDVRBrjRv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Actual correlation\n",
        "df.corr()['label_enc']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-VOc2ls_z3L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Perform feature selection\n",
        "X_train = kbest.transform(df_train.loc[:, candidate_features])\n",
        "X_test = kbest.transform(df_test.loc[:, candidate_features])\n",
        "y_train = df_train.label_enc\n",
        "y_test = df_test.label_enc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_u6jRKAW_z3N",
        "colab_type": "text"
      },
      "source": [
        "### Bayes classifier\n",
        "\n",
        "Uses Bayes Theorem and counting to compute conditional probability of each type of event.\n",
        "\n",
        "The probabilities are modeled using the gaussian probability distribution function.\n",
        "\n",
        "This is a good \"starter\" model because it is simple and interpretable. The interpretability comes from the way the predictions are generated - based on probability of feature values for a given label.\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXrEZnAh_z3N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nb = GaussianNB()\n",
        "nb.fit(X_train, y_train)\n",
        "print('Accuracy', nb.score(X_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMTS_sXK_z3O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_nb = nb.predict(X_test)\n",
        "print(classification_report(y_test, pred_nb, target_names=le.classes_))\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "sns.heatmap(confusion_matrix(y_test, pred_nb), annot=True, fmt='d',\n",
        "            xticklabels=le.classes_, yticklabels=le.classes_, ax=ax)\n",
        "ax.set_xlabel('Prediction')\n",
        "ax.set_ylabel('Truth')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcWl85NL_z3Q",
        "colab_type": "text"
      },
      "source": [
        "### Random Forest Classifier\n",
        "\n",
        "A Naive Bayes model doesn't really support tuning.  Let's try an ensemble algorithm such as Random Forest.\n",
        "\n",
        "This creates a number of decision tree classifiers on subsets of the dataset, and averages the results.\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huaCObdt_z3Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# max_depth and n_estimators limit overfitting\n",
        "rf = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)\n",
        "\n",
        "rf.fit(X_train, y_train)\n",
        "rf.score(X_test, y_test)\n",
        "\n",
        "print('Accuracy', rf.score(X_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7VOCrO6_z3R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_rf = rf.predict(X_test)\n",
        "print(classification_report(y_test, pred_rf, target_names=le.classes_))\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "sns.heatmap(confusion_matrix(y_test, pred_rf), annot=True, fmt='d',\n",
        "            xticklabels=le.classes_, yticklabels=le.classes_, ax=ax)\n",
        "ax.set_xlabel('Prediction')\n",
        "ax.set_ylabel('Truth')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3Cwba_u_z3T",
        "colab_type": "text"
      },
      "source": [
        "### Tuning\n",
        "\n",
        "We can perform a grid search both kBest and on combinations of RandomForest hyperparameters to find a better model.\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3R-4Sah_z3U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Earlier we used k=5 for kBest, now we can actually tune it while\n",
        "# training the model\n",
        "pipeline = Pipeline(steps=[\n",
        "  ('kbest', SelectKBest()),\n",
        "  ('rf', RandomForestClassifier())\n",
        "])\n",
        "\n",
        "# setup the parameter grid to search\n",
        "# the format is: stepname__parameter\n",
        "param_grid = {\n",
        "    'kbest__k' : [3, 5, 7],\n",
        "    'rf__max_depth': [5, 7, 9, 11, 13],\n",
        "    'rf__n_estimators': [75, 100, 125, 150, 175, 200]\n",
        "}\n",
        "\n",
        "X_train_before_kbest = df_train.loc[:, candidate_features]\n",
        "X_test_before_kbest = df_test.loc[:, candidate_features]\n",
        "\n",
        "# Run 4 jobs concurrently, using cross validation split of 3\n",
        "gs = GridSearchCV(pipeline, param_grid, n_jobs=4, cv=3, verbose=True, \n",
        "                  return_train_score=True)\n",
        "gs.fit(X_train_before_kbest, y_train)\n",
        "print('Accuracy', gs.score(X_test_before_kbest, y_test))\n",
        "print('Best combination', gs.best_params_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGkz66vz_z3V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get the full results\n",
        "pd.DataFrame(gs.cv_results_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rydA9S-M_z3X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_gs = gs.predict(X_test_before_kbest)\n",
        "print(classification_report(y_test, pred_gs, target_names=le.classes_))\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "sns.heatmap(confusion_matrix(y_test, pred_gs), annot=True, fmt='d',\n",
        "            xticklabels=le.classes_, yticklabels=le.classes_, ax=ax)\n",
        "ax.set_xlabel('Prediction')\n",
        "ax.set_ylabel('Truth')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gx7BTbp6y-ta",
        "colab_type": "text"
      },
      "source": [
        "<font color='red'>**Q6: What are the best k, max_depth, and n_estimators hyperparameters?**</font>\n",
        "\n",
        "(Hint: use gs.best_params_)\n",
        "\n",
        "```\n",
        "Enter your answer here:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "You can read up on max_depth and n_estimators here: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
        "\n",
        "You can read up on GridSearchCV here: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAiXI7NI_z3Z",
        "colab_type": "text"
      },
      "source": [
        "## Train using frequency domain features\n",
        "\n",
        "If time domain features aren't performing well (not really the case here), we can also try using the FFT to fit the model.\n",
        "\n",
        "1. Take rolling windows of the signal\n",
        "2. Compute spectral features based on the FFT\n",
        "3. Use both original and spectral features in our model\n",
        "\n",
        "To learn, we'll be computing the spectral features \"by hand\", but you can alternatively consider a package like tsfresh that calculates and evaluates the features automatically:\n",
        "* https://tsfresh.readthedocs.io/en/latest/text/introduction.html\n",
        "* https://tsfresh.readthedocs.io/en/latest/api/tsfresh.feature_extraction.html#tsfresh.feature_extraction.feature_calculators.fft_aggregated\n",
        "\n",
        "(Note, however, tsfresh is even more of a \"blackbox\" library)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weDcGOvF_z3Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_fft(df, window_size=1, samples_per_sec=8, smoothing=False):\n",
        "    if smoothing:\n",
        "        df_ = df.rolling(m, win_type='hamming').mean().dropna()\n",
        "    else:\n",
        "        df_ = df\n",
        "    \n",
        "    # fftpack.fftfreq returns the discrete fourier transform sample frequencies\n",
        "    # len(df_) is the window length\n",
        "    # sample spacing = 1./samples_per_sec\n",
        "    sample_freq_ = fftpack.fftfreq(len(df_), d=1./samples_per_sec)\n",
        "\n",
        "    # only take the positive frequencies (negative frequencies don't make sense)\n",
        "    pos_mask_ = sample_freq_ > 0\n",
        "\n",
        "    # wrap the result into a dataframe    \n",
        "    result_df_ = pd.DataFrame(index = sample_freq_[pos_mask_])\n",
        "    \n",
        "    # Fill in fft value for each column\n",
        "    # FFT needs to be computed per column, hence we are using a loop\n",
        "    for c in df_.columns:\n",
        "        if df_[c].dtype == float:\n",
        "            fft_ = np.abs(fftpack.fft(df_[c])) # take the magnitude\n",
        "            result_df_[c + '_fft'] = fft_[pos_mask_]\n",
        "    \n",
        "    result_df_.index.name = 'sampling_frequency'\n",
        "\n",
        "    return result_df_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIMZ8AwG2Iiw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# uncomment this to get the help information\n",
        "# fftpack.fftfreq?"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1lj4J2s_z3a",
        "colab_type": "text"
      },
      "source": [
        "Since FFT is in the frequency domain, we will need to break up the time signal into time-windows.\n",
        "\n",
        "Let's say we have time windows of 10 seconds, and we extract FFT characteristics from each window.\n",
        "\n",
        "Later on, we will be sliding the time-window across the entire duration. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBYQ1vnp_z3a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "time_window_sec = 10\n",
        "start_index = 0\n",
        "end_index = time_window_sec * samples_per_sec # 10 sec * 8 samples/sec\n",
        "\n",
        "df_time_window = df.iloc[start_index:end_index]\n",
        "fft_time_window = compute_fft(df_time_window)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "fft_time_window.plot(ax=ax) # plotting this way adds the correct labels\n",
        "\n",
        "ax.set_title(f'FFT: using {time_window_sec} second window')\n",
        "ax.set_xlabel('Frequency (Hz)')\n",
        "ax.set_ylabel('FFT magnitude')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfB90e8Q1WpK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fft_time_window"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdapNVjQ_z3b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# number of rows indicate the number of sampling frequencies\n",
        "# each entry is the absolute magnitude of the FFT at that frequency\n",
        "fft_time_window.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFWk77eN_z3f",
        "colab_type": "text"
      },
      "source": [
        "#### Spectral Centroid\n",
        "\n",
        "This corresponds to the balancing point or the center of mass of the spectral power distribution.\n",
        "This value is calculated as the weighted mean of the frequency components present in the signal:\n",
        "\n",
        "$$\\frac{\\sum{F(n) \\times n}}{\\sum F(n)}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enILwVE2_z3f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Spectral centroid is the weighted mean of the frequency components\n",
        "# fft_time_window.index are the frequencies\n",
        "spectral_centroid = np.dot(fft_time_window.index, fft_time_window) / np.sum(fft_time_window)\n",
        "spectral_centroid"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1xGtTUd_z3h",
        "colab_type": "text"
      },
      "source": [
        "#### Spectral Energy\n",
        "\n",
        "The equivalent to the energy of the signal as discussed before in\n",
        "the frequency domain is the spectral energy. This value is computed as the sum of\n",
        "the squares of the magnitude of the frequency content:\n",
        "\n",
        "$$\\sum{F(n)^2}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X66a5UYQ_z3h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Spectral energy\n",
        "np.sum(fft_time_window ** 2) # sum of squares"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxPtecnN_z3i",
        "colab_type": "text"
      },
      "source": [
        "#### Spectral Entropy\n",
        "\n",
        "This refers to the entropy of the signal in the frequency domain. The frequency content of the signal is normalized before the entropy computation.\n",
        "\n",
        "$$-\\sum{\\hat{F}(n) \\times log(\\hat{F}(n))} $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dff1yaJU_z3i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# normalise the FFT by dividing by sum\n",
        "normalized_fft_time_window = fft_time_window / np.sum(fft_time_window)\n",
        "\n",
        "# compute entropy\n",
        "-np.sum(normalized_fft_time_window * np.log(normalized_fft_time_window))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGBngAMc_z3k",
        "colab_type": "text"
      },
      "source": [
        "### Computing Spectal Features\n",
        "\n",
        "We'll wrap the computation into a function and then call that function using the pd.DataFrame.rolling() method.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phy4aMHD_z3k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_fft_series(df, window_size=1, samples_per_sec=8, smoothing=False):\n",
        "    if smoothing:\n",
        "        df_ = df.rolling(m, win_type='hamming').mean().dropna()\n",
        "    else:\n",
        "        df_ = df\n",
        "    \n",
        "    sample_freq_ = fftpack.fftfreq(len(df_), d=1./samples_per_sec)\n",
        "    pos_mask_ = sample_freq_ > 0\n",
        "    \n",
        "    fft_ = np.abs(fftpack.fft(df_))\n",
        "    result_df_ = pd.Series(index=sample_freq_[pos_mask_], data=fft_[pos_mask_])\n",
        "    result_df_.index.name = 'sampling_frequency'\n",
        "\n",
        "    return result_df_    \n",
        "\n",
        "def spectral_centroid(df):\n",
        "    fft_ = compute_fft_series(df, window_size=1, samples_per_sec=samples_per_sec, smoothing=False)\n",
        "    return np.dot(fft_.index, fft_) / np.sum(fft_)\n",
        "\n",
        "def spectral_energy(df):\n",
        "    fft_ = compute_fft_series(df, window_size=1, samples_per_sec=samples_per_sec, smoothing=False)\n",
        "    return np.sum(fft_ ** 2)\n",
        "\n",
        "def spectral_entropy(df):\n",
        "    fft_ = compute_fft_series(df, window_size=1, samples_per_sec=samples_per_sec, smoothing=False)\n",
        "    normalized_fft_ = fft_ / np.sum(fft_)\n",
        "    return -np.sum(normalized_fft_ * np.log(normalized_fft_))\n",
        "\n",
        "# Example usage:\n",
        "window_size_secs = 10\n",
        "\n",
        "# raw = False applies to a pandas Series\n",
        "# df['ay'].rolling(window_size_secs * samples_per_sec).apply(spectral_centroid, raw=False).dropna().head(10)\n",
        "# df['ay'].rolling(window_size_secs * samples_per_sec).apply(spectral_energy, raw=False).dropna().head(10)\n",
        "df['ay'].rolling(window_size_secs * samples_per_sec).apply(spectral_entropy, raw=False).dropna().head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJgWe9IZ_z3l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# columns to generate spectral features from\n",
        "# In general, you want to use the raw columns instead of the discretised columns, because \n",
        "# there is a loss of information from the discretised columns.\n",
        "float_columns = ['ax', 'ay', 'az', 'temp', 'EDA', 'SpO2', 'hr']\n",
        "\n",
        "# This can take a while to run... because we do an FFT per column, 3 times\n",
        "# While groupby supports multiple functions, groupby does not do rolling windows, \n",
        "# so we have to call rolling 3 times\n",
        "for f in float_columns:\n",
        "    df[f + '_sc'] = df[f].rolling(window_size_secs * samples_per_sec).apply(spectral_centroid, raw=False)\n",
        "    df[f + '_sen'] = df[f].rolling(window_size_secs * samples_per_sec).apply(spectral_energy, raw=False)\n",
        "    df[f + '_se'] = df[f].rolling(window_size_secs * samples_per_sec).apply(spectral_entropy, raw=False)\n",
        "\n",
        "df.dropna(inplace=True)\n",
        "df.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPisPEDb_z3m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# move the label column to the front so that it is easy to see\n",
        "new_cols = ['label', 'label_enc'] + df.columns.drop(['label', 'label_enc']).tolist()\n",
        "\n",
        "df = df[new_cols]\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sbh28ufd_z3n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.corr()['label_enc']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHcvzDaG8IRF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# filter for higher correlations\n",
        "mask = abs(df.corr()['label_enc']) > 0.2\n",
        "df.corr()['label_enc'][mask]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXGcbIOT5IKR",
        "colab_type": "text"
      },
      "source": [
        "### Shuffle and Train-test split\n",
        "\n",
        "With the extra spectral features, we will see if we can improve the simple (Bayes) model.\n",
        "\n",
        "As stated before, the Bayes model is more interpretable because it is probability-based."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uozEvGg843uz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train test split\n",
        "df_train_sp, df_test_sp = train_test_split(df, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsTTeGyu8lep",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# exclude the label, encoded label, and the quantised intervals\n",
        "candidate_features_sp = df.columns.drop(['label', 'label_enc', 'ax_q10', 'ay_q10', 'az_q10'])\n",
        "candidate_features_sp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wK4Tb5Ps_z3o",
        "colab_type": "text"
      },
      "source": [
        "### Bayes classifier (revisited)\n",
        "\n",
        "<font color='red'>**Q7: Apply the new features train a new Bayes Classifier. Use k=8 for SelectKBest. What is the new accuracy score and classification report?**</font>\n",
        "\n",
        "```\n",
        "Enter your answer here:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcmnKdEc_z3v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1. apply SelectKBest, with k=8 (giving some room as we have more features)\n",
        "kbest_sp = SelectKBest(k=8)\n",
        "kbest_sp.fit(df_train_sp.loc[:, candidate_features_sp], df_train_sp.label_enc)\n",
        "\n",
        "# Perform feature selection\n",
        "X_train_sp = kbest_sp.transform(df_train_sp.loc[:, candidate_features_sp])\n",
        "y_train_sp = df_train_sp.label_enc\n",
        "X_test_sp = kbest_sp.transform(df_test_sp.loc[:, candidate_features_sp])\n",
        "y_test_sp = df_test_sp.label_enc\n",
        "\n",
        "# 2. apply GaussianNB on the selected 8 features\n",
        "nb_sp = GaussianNB()\n",
        "#\n",
        "# Enter your code below\n",
        "#\n",
        "\n",
        "\n",
        "\n",
        "# 3. compute the accuracy score on the test set\n",
        "#\n",
        "# Enter your code below\n",
        "#\n",
        "\n",
        "\n",
        "\n",
        "# 4. compute the classification report and confusion matrix on the test set\n",
        "#\n",
        "# Enter your code below\n",
        "#\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pI7Gto8__z3w",
        "colab_type": "text"
      },
      "source": [
        "## Decision Boundaries\n",
        "\n",
        "(You can never be done with analysis).\n",
        "\n",
        "Let's see if we can compare the decision boundaries, to get a better sense of how the features are spread out.\n",
        "\n",
        "1. Perform 2-D PCA so that we can visualise all the features in a 2-D plot\n",
        "2. Create a mesh grid, colour regions based on predictions from the classifier\n",
        "3. Repeat step 2 for each classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vVWPBXy_z3w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# PCA presumes zero centered mean, so we scale\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_sp)\n",
        "X_test_scaled = scaler.transform(X_test_sp)\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "\n",
        "Z_train_pca = pca.fit_transform(X_train_scaled)\n",
        "Z_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "# Train and test sets are shuffled by train_test_split before the split,\n",
        "# so we need to concatenate instead of using X and y.\n",
        "# Also, we want to use the scaled version of X\n",
        "Z_pca = np.concatenate((Z_train_pca, Z_test_pca), axis=0)\n",
        "y_all = np.concatenate((y_train_sp, y_test_sp), axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyY0czee_z3y",
        "colab_type": "text"
      },
      "source": [
        "### Plot data points"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cqEvNqI_z3y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(15, 10))\n",
        "\n",
        "for i in range(0, len(le.classes_)):\n",
        "    # y_train == i filters a subset of rows where y_train is i    \n",
        "    # The ..., 0] refers to the first principal component\n",
        "    # The ..., 1] refers to the second principal component\n",
        "    ax.scatter(Z_pca[y_all == i, 0], Z_pca[y_all == i, 1], label=le.classes_[i])\n",
        "ax.legend()\n",
        "\n",
        "ax.set_title('PCA plot of dataset including Spectral Features')\n",
        "ax.set_xlabel('Principal component 1')\n",
        "ax.set_ylabel('Principal component 2')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6mIQNJt_z30",
        "colab_type": "text"
      },
      "source": [
        "### Plot decision boundaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZn2COrw_z30",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_decision_boundaries(ax, title, clf, data, target, step_size=.02):\n",
        "    \"\"\"Plots the decision boundaries for a fitted classifier model\n",
        "    Args:\n",
        "        ax: subplot axis\n",
        "        title: subplot title\n",
        "        clf: a fitted sklearn classification model\n",
        "        data: 2-dimensional input data\n",
        "        target: the target truth values (y)\n",
        "        step_size: Step size of the mesh. Decrease to increase the quality of the plot.\n",
        "        \n",
        "    Globals:\n",
        "        pca: fitted PCA transformer for projecting 2-dimensional to original X dimension\n",
        "        scaler: fitted StandardScaler for unscaling the X back to original scale\n",
        "        le: fitted LabelEncoder for mapping to class labels\n",
        "\n",
        "    Based on: http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html\n",
        "    \"\"\"\n",
        "    # Generate a mesh of many points, this will become regions when coloured\n",
        "    x_min_, x_max_ = data[:, 0].min() - 1, data[:, 0].max() + 1\n",
        "    y_min_, y_max_ = data[:, 1].min() - 1, data[:, 1].max() + 1\n",
        "    xx_, yy_ = np.meshgrid(np.arange(x_min_, x_max_, step_size), np.arange(y_min_, y_max_, step_size))\n",
        "\n",
        "    # Make 2 columns (principal component 1, principal component 2)\n",
        "    zz_ = np.c_[xx_.ravel(), yy_.ravel()]\n",
        "    \n",
        "    # We need to inverse both PCA and scaling because clf is trained on unscaled and un-PCA'ed data\n",
        "    z_ = pca.inverse_transform(zz_) # inverse-PCA\n",
        "    z_ = scaler.inverse_transform(z_) # unscale\n",
        "    \n",
        "    # Obtain labels for each point in mesh using the trained model\n",
        "    pred_ = clf.predict(z_)\n",
        "\n",
        "    # Put the result into a color plot\n",
        "    pred_ = pred_.reshape(xx_.shape)\n",
        "\n",
        "    # Plot the decision boundary. Assign a colour based on prediction\n",
        "    ax.imshow(pred_, interpolation='nearest',\n",
        "              extent=(xx_.min(), xx_.max(), yy_.min(), yy_.max()),\n",
        "              cmap=plt.cm.Pastel2,\n",
        "              aspect='auto', origin='lower')\n",
        "\n",
        "    # Plot the points. Assign a colour based on truth\n",
        "    # Made the points smaller and more transparent so that the boundaries are still visible.\n",
        "    for i in range(0, len(le.classes_)):\n",
        "        ax.scatter(data[target == i, 0], data[target == i, 1], label=le.classes_[i],\n",
        "                   alpha=.6)\n",
        "\n",
        "    ax.set(title=title, xlim=(x_min_, x_max_), ylim=(y_min_, y_max_),\n",
        "           xlabel='Principal component 1', ylabel='Principal component 2')\n",
        "    ax.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "DSpnDbbl_z32",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Just plot the test set to speed things up\n",
        "fig, ax1 = plt.subplots(figsize=(15, 10))\n",
        "plot_decision_boundaries(ax1, 'Naive Bayes Classifier Decision Boundaries', nb_sp, Z_test_pca, y_test_sp)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIT8yUGp_z33",
        "colab_type": "text"
      },
      "source": [
        "Naive Bayes is a simple model that produces gaussian-shaped decision boundaries. \n",
        "\n",
        "Note: The boundaries are also projected in 2-D, so it looks like there are more mistakes here, but each actual boundary is N dimension."
      ]
    }
  ]
}
