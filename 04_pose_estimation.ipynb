{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pose Estimation\n",
    "\n",
    "In this notebook, we will explore OpenPose, a pose estimation library. OpenPose detects human body keypoints using single images.\n",
    "\n",
    "https://github.com/CMU-Perceptual-Computing-Lab/openpose\n",
    "\n",
    "It uses a multi-person pose estimation model. Given an input colour image, the model returns two outputs:\n",
    "1. Confidence maps of body part locations (\"keypoints\")\n",
    "2. Part affinity heatmaps for each keypoint pair (\"associations\" between body parts). \n",
    "\n",
    "![multipose](https://github.com/ZheC/Realtime_Multi-Person_Pose_Estimation/raw/master/readme/arch.png)\n",
    "\n",
    "More details on the model are here: https://github.com/ZheC/Realtime_Multi-Person_Pose_Estimation\n",
    "\n",
    "## Example Use Cases\n",
    "\n",
    "Pose estimation models can be used in video pre-processing to extract poses for further analysis. For example, in this paper, the sequence of poses are fed into a Hidden Markov Model for predicting future poses. Other use cases are gaming, medical diagnosis, robotic control, driver assistant, etc (see: https://en.wikipedia.org/wiki/Articulated_body_pose_estimation)\n",
    "\n",
    "![pose-dmm-dicta-17](assets/pose-dmm-dicta-17.png)\n",
    "\n",
    "Source: Toyer et. al., Human Pose Forecasting via Deep Markov Models, 2017\n",
    "\n",
    "### Other Solutions\n",
    "\n",
    "https://github.com/facebookresearch/VideoPose3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running OpenPose from the Command Line\n",
    "\n",
    "We will be exploring the OpenPose Demo command line application to extract poses from a video file.\n",
    "\n",
    "The poses will be saved to JSON."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup (Windows only)\n",
    "\n",
    "1. Download OpenPose binaries from https://github.com/CMU-Perceptual-Computing-Lab/openpose/releases. \n",
    "   Example: openpose-1.5.1-binaries-win64-only_cpu-python-flir-3d.zip\n",
    "\n",
    "2. Unzip the file\n",
    "\n",
    "3. The pre-trained models are not included in the zip file because they are quite large. Download them:\n",
    "\n",
    "```\n",
    "cd openpose-1.5.1-win64-only_cpu-python-flir-3d\\openpose\\models\n",
    "\n",
    "getModels.bat\n",
    "```\n",
    "\n",
    "The download should take about 5-10 minutes.\n",
    "\n",
    "#### Non-Windows?\n",
    "\n",
    "Unfortunately, installation for non-Windows involves building from source. To save time, you can partner with a classmate who has a Windows machine. \n",
    "\n",
    "Later, if you plan to do more work using OpenPose, you may wish to try these instructions at home (in your own time): https://github.com/CMU-Perceptual-Computing-Lab/openpose/blob/master/doc/installation.md#openpose-from-other-projects-ubuntu-and-mac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Keypoints from Video (Windows only)\n",
    "\n",
    "1. <font color='red'>**[Important] run the demo from the top level openpose folder, not the subfolders:**</font>\n",
    "\n",
    "```\n",
    "cd openpose-1.5.1-win64-only_cpu-python-flir-3d\\openpose\n",
    "\n",
    "bin\\OpenPoseDemo.exe --video examples\\media\\video.avi --part_candidates --write_json output\n",
    "```\n",
    "\n",
    "   The above command runs the demo for face and hand keypoint detection, and writes the results as JSON format in a folder called `output`. The `part_candidates` option will include predicted poses.\n",
    "\n",
    "\n",
    "2. The video will run very slowly because this is the CPU version. \n",
    "\n",
    "    Wait a few minutes to see keypoints appearing in the `output` folder.  There's no easy way to stop the demo. <font color='red'>**Stop the demo after about 3-4 frames, because it consumes a lot of memory (use Ctrl+C from Command Prompt).**</font>\n",
    "\n",
    "    If you have an NVidia GPU, you may want to download the GPU version (which run significantly faster and is recommended).\n",
    "\n",
    "\n",
    "3. To optionally capture in COCO format, which captures more keypoints:\n",
    "\n",
    "```\n",
    "bin\\OpenPoseDemo.exe --video examples\\media\\video.avi --write_coco_json output.coco.json\n",
    "```\n",
    "\n",
    "\n",
    "4. Other options are documented at https://github.com/CMU-Perceptual-Computing-Lab/openpose/blob/master/doc/demo_overview.md, and also here:\n",
    "\n",
    "```\n",
    "bin\\OpenPoseDemo.exe --help\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring keypoints\n",
    "\n",
    "The following section shows how to take the keypoints and plot them. \n",
    "\n",
    "For this section, we will use pre-extracted keypoint data in the HSS github repository, under the `data\\openpose` folder.  This is the same set of keypoints extracted from the earlier video (but run from a GPU to speed things up).\n",
    "\n",
    "The output format is documented here:\n",
    "https://github.com/CMU-Perceptual-Computing-Lab/openpose/blob/master/doc/output.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import paired_distances\n",
    "\n",
    "# requires: conda install opencv\n",
    "import cv2\n",
    "\n",
    "plt.style.use('seaborn-white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load the 200th frame\n",
    "frame = json.load(open('./data/openpose/video_000000000200_keypoints.json', 'rb'))\n",
    "frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count how many people\n",
    "len(frame['people'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what keypoints are logged for each person\n",
    "frame['people'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# format is x1, y1, c1, x2, y2, c2, ...\n",
    "# x, y coordinates with a confidence\n",
    "frame['people'][0]['pose_keypoints_2d']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keypoints_to_dataframe(keypoints):\n",
    "    \"\"\"Converts a flat keypoints list (x1, y1, c1, x2, y2, c2) into a pandas DataFrame\"\"\"\n",
    "    return pd.DataFrame({'x': keypoints[::3], 'y': keypoints[1::3], 'c': keypoints[2::3]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drawing Skeletons\n",
    "\n",
    "The keypoints follow the BODY_25 format, which can be used to connect the points to make a skeleton\n",
    "\n",
    "<img src='https://github.com/CMU-Perceptual-Computing-Lab/openpose/raw/master/doc/media/keypoints_pose_25.png' width=300/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://github.com/CMU-Perceptual-Computing-Lab/openpose/blob/master/doc/output.md#pose-output-format-coco\n",
    "Pose_part_pairs = [\n",
    "    (1,8), (1,2), (1,5), (2,3), (3,4), (5,6), (6,7), (8,9), (9,10), (10,11),\n",
    "    (8,12), (12,13), (13,14), (1,0), (0,15), (15,17), (0,16), (16,18), (2,17), (5,18),\n",
    "    (14,19), (19,20), (14,21), (11,22), (22,23), (11,24)\n",
    "]\n",
    "\n",
    "def draw_skeleton(ax, df):\n",
    "    for p, q in Pose_part_pairs:\n",
    "        if df.x[p] != 0 and df.x[q] != 0 and df.y[p] != 0 and df.y[q] != 0:\n",
    "            ax.plot([df.x[p], df.x[q]], [df.y[p], df.y[q]], color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "video = cv2.VideoCapture('./data/openpose/video.avi')\n",
    "\n",
    "frame = json.load(open(f'./data/openpose/video_000000000100_keypoints.json', 'rb'))\n",
    "df = keypoints_to_dataframe(frame['people'][0]['pose_keypoints_2d'])\n",
    "\n",
    "for i in range(100):\n",
    "    _, image = video.read()\n",
    "_, image1 = video.read()\n",
    "\n",
    "# different size dots according to probability\n",
    "ax.scatter(df.x, df.y, s=df.c*10, color='yellow')\n",
    "ax.set(title='Frame 100, Person 0')\n",
    "\n",
    "# openCV uses BGR, convert to RGB\n",
    "ax.imshow(cv2.cvtColor(image1, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "draw_skeleton(ax, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting a Scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=3, ncols=2, figsize=(15, 15), sharex=True, sharey=True)\n",
    "ax = ax.flatten()\n",
    "\n",
    "video = cv2.VideoCapture('./data/openpose/video.avi')\n",
    "\n",
    "# fast forward\n",
    "for i in range(100):\n",
    "    _, image = video.read()\n",
    "_, image = video.read()\n",
    "    \n",
    "for i in range(min(len(frame['people']), len(ax))):\n",
    "    df = keypoints_to_dataframe(frame['people'][i]['pose_keypoints_2d'])\n",
    "    \n",
    "    # different size dots according to probability\n",
    "    ax[i].scatter(df.x, df.y, s=df.c*10, color='yellow')\n",
    "    ax[i].set(title=f'Person {i}', xlabel='x-coord', ylabel='y-coord')\n",
    "    \n",
    "    # openCV uses BGR, convert to RGB\n",
    "    ax[i].imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    \n",
    "    draw_skeleton(ax[i], df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tracking the trajectory of 1 person is harder because the people are not returned in the same order.\n",
    "\n",
    "We will need to perform some sort of distance measurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame1 = json.load(open('./data/openpose/video_000000000150_keypoints.json', 'rb'))\n",
    "frame2 = json.load(open('./data/openpose/video_000000000151_keypoints.json', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = keypoints_to_dataframe(frame1['people'][0]['pose_keypoints_2d'])\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = keypoints_to_dataframe(frame2['people'][0]['pose_keypoints_2d'])\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously different person, because the coordinates jumped significantly between frames.\n",
    "\n",
    "Let's compute the center of mass of the points, and then use that \"centroid\" for each person to find the closest centroid in the next frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_centroid(coordinates, threshold=0.1):\n",
    "    \"\"\"Computes the centroid of a given 2 dimensional vector\"\"\"\n",
    "    x = coordinates[coordinates.c > threshold].x\n",
    "    y = coordinates[coordinates.c > threshold].y\n",
    "    \n",
    "    return [sum(x)/len(x), sum(y)/len(y)]\n",
    "\n",
    "get_centroid(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with this, we can go through each frame and compute the centroids\n",
    "\n",
    "def get_centroids(frame):\n",
    "    \"\"\"Returns the centroid for each person as a list of (x, y) coordinates\"\"\"\n",
    "    return np.array([get_centroid(keypoints_to_dataframe(person['pose_keypoints_2d'])) for person in frame['people']])\n",
    "\n",
    "get_centroids(frame1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest_index(centroid, other_frame):\n",
    "    \"\"\"Find closest index in other_frame from a given centroid\"\"\"\n",
    "    other_centroids = get_centroids(other_frame)\n",
    "    return np.argmin(paired_distances(np.ones(other_centroids.shape) * centroid, other_centroids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[get_closest_index(centroid, frame2) for centroid in get_centroids(frame1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Person 0 in Frame 150 is closest to Person 3 in Frame 151."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 10), sharex=True, sharey=True)\n",
    "ax = ax.flatten()\n",
    "\n",
    "video = cv2.VideoCapture('./data/openpose/video.avi')\n",
    "\n",
    "# fast forward\n",
    "for i in range(150):\n",
    "    _, image = video.read()\n",
    "_, image1 = video.read() # frame 150\n",
    "_, image2 = video.read() # frame 151\n",
    "\n",
    "df1 = keypoints_to_dataframe(frame1['people'][0]['pose_keypoints_2d'])\n",
    "ax[0].scatter(df1.x, df1.y, s=df1.c*10, color='yellow')\n",
    "ax[0].set(title='Frame 150, Person 0')\n",
    "ax[0].imshow(cv2.cvtColor(image1, cv2.COLOR_BGR2RGB))\n",
    "draw_skeleton(ax[0], df1)\n",
    "\n",
    "df2 = keypoints_to_dataframe(frame2['people'][3]['pose_keypoints_2d'])\n",
    "ax[1].scatter(df2.x, df2.y, s=df2.c*10, color='yellow')\n",
    "ax[1].set(title='Frame 151, Person 3')\n",
    "ax[1].imshow(cv2.cvtColor(image2, cv2.COLOR_BGR2RGB))\n",
    "draw_skeleton(ax[1], df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extending this further, you should be able to now trace the trajectory of the person\n",
    "# We will do this every 10 frames\n",
    "\n",
    "fig, ax = plt.subplots(nrows=3, ncols=2, figsize=(15, 15), sharex=True, sharey=True)\n",
    "ax = ax.flatten()\n",
    "\n",
    "offset = 100\n",
    "interval = 10\n",
    "\n",
    "video = cv2.VideoCapture('./data/openpose/video.avi')\n",
    "\n",
    "# fast forward\n",
    "for i in range(offset):\n",
    "    _, image = video.read()\n",
    "\n",
    "frame = json.load(open(f'./data/openpose/video_000000000{offset}_keypoints.json', 'rb'))\n",
    "df = keypoints_to_dataframe(frame['people'][0]['pose_keypoints_2d'])\n",
    "centroid = get_centroid(df)\n",
    "\n",
    "for i in range(0, 60, interval):\n",
    "    _, image = video.read()\n",
    "        \n",
    "    frame = json.load(open(f'./data/openpose/video_000000000{offset+i}_keypoints.json', 'rb'))\n",
    "    index = get_closest_index(centroid, frame) # find the closest person\n",
    "    \n",
    "    # load keypoints for the closest person\n",
    "    df = keypoints_to_dataframe(frame['people'][index]['pose_keypoints_2d'])\n",
    "    \n",
    "    axis = ax[i//interval]\n",
    "    axis.scatter(df.x, df.y, s=df.c*10, color='yellow')\n",
    "    axis.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    axis.set(title=f'Frame {offset+i}')\n",
    "    draw_skeleton(axis, df)\n",
    "    \n",
    "    for j in range(interval):\n",
    "        _, image = video.read() # fast forward\n",
    "        \n",
    "    centroid = get_centroid(df) # update centroid since person may have moved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice\n",
    "\n",
    "1. Download your own sample video. You can try a site such as: https://www.videvo.net/free-stock-footage/people/ for free videos of people.\n",
    "\n",
    "2. Generate keypoints from the video using OpenPose. You do not need the COCO format. Capture at least 10 frames with people.\n",
    "\n",
    "```\n",
    "bin\\OpenPoseDemo.exe --part_candidates --write_json c:\\temp\\output --video c:\\temp\\myvideo.mp4\n",
    "```\n",
    "\n",
    "3. Use the keypoints to trace the trajectory. You can start with the cell below, which performs trajectory tracking for 6 frames, starting with offset 2. Depending on when people show up in the video, you will need to modify the frame offset.\n",
    "\n",
    "    The result can look something like this:\n",
    "    ![image](./data/openpose/hkstreet.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 frames\n",
    "fig, ax = plt.subplots(nrows=3, ncols=2, figsize=(15, 15), sharex=True, sharey=True)\n",
    "ax = ax.flatten()\n",
    "\n",
    "# modify this to start at the frame where people are in the video\n",
    "offset = 2\n",
    "interval = 1\n",
    "\n",
    "# path to the video. avi, mp4 have been tested\n",
    "# Example (https://www.videvo.net/video/pedestrians-crossing-street-in-hong-kong/8152/)\n",
    "# video_path = 'd:/temp/example/180301_03_A_CausewayBay_06.mp4'\n",
    "video_path = 'path/to/my/video.mp4'\n",
    "\n",
    "# keypoints files usually contain the video name as prefix\n",
    "# Example:\n",
    "# keypoints_path = 'd:/temp/example/output/180301_03_A_CausewayBay_06'\n",
    "keypoints_path = 'path/to/my/output/video'\n",
    "\n",
    "video = cv2.VideoCapture(video_path)\n",
    "\n",
    "# fast forward to offset\n",
    "for i in range(offset):\n",
    "    _, image = video.read()\n",
    "\n",
    "frame = json.load(open(f'{keypoints_path}_00000000000{offset}_keypoints.json', 'rb'))\n",
    "df = keypoints_to_dataframe(frame['people'][0]['pose_keypoints_2d'])\n",
    "centroid = get_centroid(df)\n",
    "\n",
    "for i in range(0, 6, interval):\n",
    "    _, image = video.read()\n",
    "        \n",
    "    frame = json.load(open(f'{keypoints_path}_00000000000{offset+i}_keypoints.json', 'rb'))\n",
    "    index = get_closest_index(centroid, frame) # find the closest person\n",
    "    \n",
    "    # load keypoints for the closest person\n",
    "    df = keypoints_to_dataframe(frame['people'][index]['pose_keypoints_2d'])\n",
    "    \n",
    "    axis = ax[i//interval]\n",
    "    axis.scatter(df.x, df.y, s=df.c*10, color='yellow')\n",
    "    axis.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    axis.set(title=f'Frame {offset+i}')\n",
    "    draw_skeleton(axis, df)\n",
    "    \n",
    "    centroid = get_centroid(df) # update centroid since person may have moved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
